# Detla Lake 介绍

## 简单了解 Parquet

在介绍 Delta Lake 之前，首先我们要简单了解一下 Parquet 文件格式。

Parquet 仅仅是一种存储格式，它是语言、平台无关的，并且不需要和任何一种数据处理框架绑定，目前能够和 Parquet 适配的组件包括下面这些，可以看出基本上通常使用的查询引擎和计算框架都已适配，并且可以很方便的将其它序列化工具生成的数据转换成 Parquet 格式：

- 查询引擎: Hive, Impala, Pig, Presto, Drill, Tajo, HAWQ, IBM Big SQL
- 计算框架: MapReduce, Spark, Cascading, Crunch, Scalding, Kite
- 数据模型: Avro, Thrift, Protocol Buffers, POJOs

一般来讲，我们可以将数据分为两种，一种是二维表结构的数据，一种是嵌套结构的数据。嵌套型数据比如我们常常使用的 json，一般日志就是嵌套型数据。

二维表结构的数据其实也是我们将原始数据经过一些处理后形成的，在大数据领域，我们遇到更多的可能是嵌套型的数据。

首先我们来看一下数据的问题，假设一条日志里也许有几十个属性，这种数据转为表结构没什么问题，我们只需要做一张有几十列的表格，然后将日志插入进去即可。但是如果这日志里的一些不是基本类型，而是嵌套的内容。就像是一个 json 对象里有一个属性又嵌套了一个 json 对象（map 里嵌套 map，array 里嵌套 array等等）。这样我们就没有办法简单的做成一张二维表了，或许我们会想到把嵌套的对象单独做一张表，两张表做关联即可。但是如果嵌套的对象特别多如何，或者说，给你一个非常复杂的 json 对象，需要你存入关系型数据库，工作量将会非常庞大。

可以想到，嵌套型数据很难转化为二维表结构数据，二维表结构数据也同样很难转化为嵌套型数据。但是世界上产生的数据你并不能控制其格式。举个例子，我们收集了很多 csv 格式的数据，也收集了 json 格式的数据。被收集的数据都以它们原本的格式放到了 HDFS 上。接下来，当我们要分析这些数据的时候，难道我们针对不同的格式使用不同的产品吗，就算是相同的产品，也要用不同的接口吧。更何况这些原始数据内部会有大量的重复数据，会造成存储资源浪费。

总之，Parquet 存储格式可以解决上述问题，我们不需要考虑 Parquet 格式的具体原理，我们只需要直到，Parquet 既可以读取成结构化数据，也可以读取成嵌套型数据，反过来这两种数据也可以都存储为 Parquet 格式。所以说 Parquet 是语言、平台无关的一种存储格式。

## 什么是 Detla Lake

从名字上来讲，首先想到这可能是一个数据湖。
